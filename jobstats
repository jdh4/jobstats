#!/usr/bin/python3
import argparse
import csv
import datetime
import os
import subprocess
import sys
import time
import requests
import json
import base64
import gzip
from blessed import Terminal

# for convenience
DEVNULL = open(os.devnull, 'w')
# it's what we need to get unix times
os.environ['SLURM_TIME_FORMAT']="%s"

# prometheus server to query
PROM_SERVER="http://vigilant:8480"

# class that gets and holds per job prometheus statistics
class JobStats:
    # threshold values for red versus black notes
    gpu_utilization_red = 15
    gpu_utilization_black = 25
    cpu_utilization_red = 65
    cpu_utilization_black = 80
    min_runtime_seconds = 300
    min_memory_usage = 70

    # initialize basic job stats, can be called either with those stats
    # provided and if not it will fetch them
    def __init__(self, jobid=None, jobidraw=None, start=None, end=None, gpus=None, cluster=None, debug=False):
        self.cluster = cluster
        self.debug = debug
        self.sp_node = {}
        if jobidraw == None:
            self.jobid = jobid
            if not self.__get_job_info():
                if self.state == "PENDING":
                    self.error("Failed to get details for job %s since it is a PENDING job." % jobid)
                else:
                    self.error("Failed to get details for job %s." % jobid)
        else:
            # no way to enter this branch yet
            if jobid == None:
                jobid = jobidraw
            self.jobid = jobid
            self.jobidraw = jobidraw
            self.start = start
            self.end = end
            self.gpus = gpus
            self.data = None
        self.diff = self.end - self.start
        # for tiger data is collected as tiger but slurm cluster name is tiger2
        if self.cluster == "tiger2":
            self.cluster = "tiger"
        if self.debug:
            print("DEBUG: jobid=%s, jobidraw=%s, start=%s, end=%s, gpus=%s, diff=%s, cluster=%s, data=%s, timelimitraw=%s" % 
                  (self.jobid,self.jobidraw,self.start,self.end,self.gpus,self.diff,self.cluster,self.data,self.timelimitraw))
        if self.data != None and self.data.startswith('JS1:') and len(self.data)>10:
            try:
                t = json.loads(gzip.decompress(base64.b64decode(self.data[4:])))
                self.sp_node = t["nodes"]
            except Exception as e:
                print("ERROR: %s" %e)
        if not self.sp_node:
            # call prometheus to get detailed statistics
            self.get_job_stats()

    def nodes(self):
        return self.sp_node

    def jobid(self):
        return self.jobidraw

    def diff(self):
        return self.diff

    def gpus(self):
        return self.gpus

    # report an error on stderr and fail
    def error(self, msg):
        if __name__ == "__main__":
            sys.stderr.write("%s\n" % msg)
            sys.exit(1)
        else:
            raise Exception(msg)

    # Get basic info from sacct and set instance variables
    def __get_job_info(self):
        # jobname must be last field next line to handle "|" chars later on
        cmd = ["sacct", "-P", "-X", "-o",
               "jobidraw,start,end,cluster,reqtres,admincomment,user,account,state,nnodes,ncpus,reqmem,qos,partition,timelimitraw,jobname",
               "-j", self.jobid]
        if self.cluster:
            cmd += ["-M",self.cluster]
        self.start = None
        self.end = None
        try:
            for i in csv.DictReader(subprocess.check_output(cmd,stderr=DEVNULL).decode("utf-8").split('\n'), delimiter='|'):
                self.jobidraw     = i.get('JobIDRaw',None)
                self.start        = i.get('Start',None)
                self.end          = i.get('End',None)
                self.cluster      = i.get('Cluster',None)
                tres              = i.get('ReqTRES',None)
                self.data         = i.get('AdminComment',None)
                self.user         = i.get('User',None)
                self.account      = i.get('Account',None)
                self.state        = i.get('State',None)
                self.timelimitraw = i.get('TimelimitRaw',None)
                self.nnodes       = i.get('NNodes',None)
                self.ncpus        = i.get('NCPUS',None)
                self.reqmem       = i.get('ReqMem',None)
                self.qos          = i.get('QOS',None)
                self.partition    = i.get('Partition',None)
                self.jobname      = i.get('JobName',None)  # if "|" in jobname then will be truncated
        except Exception as e:
            self.error("Failed to lookup jobid %s" % jobid)
        self.gpus = 0
        if tres != None and 'gres/gpu=' in tres and 'gres/gpu=0,' not in tres:
            for part in tres.split(","):
                if "gres/gpu=" in part:
                    self.gpus = int(part.split("=")[-1])
 
        if self.timelimitraw.isnumeric():
            self.timelimitraw = int(self.timelimitraw)
        if "CANCEL" in self.state:
          self.state = "CANCELLED"
        if len(self.jobname) > 32:
            self.jobname = self.jobname[:32] + "..."

        # currently running jobs will have Unknown as time
        if self.end == 'Unknown':
            self.end = time.time()
        else:
            if self.end.isnumeric():
                self.end = int(self.end)
            else:
                return False
        if self.start.isnumeric():
            self.start = int(self.start)
            return True
        else:
            return False

    # extract info out of what was returned
    # sp = hash indexed by node
    # d  = data returned from prometheus
    # n  = what name to give this data
    #{'metric': {'__name__': 'cgroup_memory_total_bytes', 'cluster': 'stellar', 'instance': 'stellar-m02n30:9306', 'job': 'Stellar Nodes', 'jobid': '50783'}, 'values': [[1629592582, '536870912000']]}
    # or
    #{'metric': {'cluster': 'stellar', 'instance': 'stellar-m06n4:9306', 'job': 'Stellar Nodes', 'jobid': '50783'}, 'value': [1629592575, '190540828672']}
    def get_data_out(self, d, n):
        if 'data' in d:
            j = d['data']['result']
            for i in j:
                node=i['metric']['instance'].split(':')[0]
                minor = i['metric'].get('minor_number', None)
                if 'value' in i:
                    v=i['value'][1]
                if 'values' in i:
                    v=i['values'][0][0]
                # trim unneeded precision
                if '.' in v:
                    v = round(float(v), 1)
                else:
                    v = int(v)
                if node not in self.sp_node:
                    self.sp_node[node] = {}
                if minor != None:
                    if n not in self.sp_node[node]:
                        self.sp_node[node][n] = {}
                    self.sp_node[node][n][minor] = v
                else:
                    self.sp_node[node][n] = v

    def get_data(self, where, query):
        # run a query against prometheus
        def __run_query(q, start=None, end=None, time=None, step=60):
            params = { 'query': q, }
            if start:
                params['start'] = start
                params['end'] = end
                params['step'] = step
                qstr = 'query_range'
            else:
                qstr = 'query'
                if time:
                    params['time'] = time
            response = requests.get('{0}/api/v1/{1}'.format(PROM_SERVER, qstr), params)
            return response.json()
        
        expanded_query = query%(self.cluster, self.jobidraw, self.diff)
        if self.debug:
            print("DEBUG: query=%s, time=%s" %(expanded_query,self.end))
        try:
            j = __run_query(expanded_query, time=self.end)
        except Exception as e:
            self.error("ERROR: Failed to query jobstats database, got error: %s:" % e)
        if self.debug:
            print("DEBUG: query result=%s" % j)
        if j["status"] == 'success':
            self.get_data_out(j, where)
        elif j["status"] == 'error':
            self.error("ERROR: Failed to get run query %s with time %s, error: %s" % (expanded_query, self.end, j["error"]))
        else:
            self.error("ERROR: Unknown result when running query %s with time %s, full output: %s" %(expanded_query, self.end, j))

    def get_job_stats(self):
        # query CPU and Memory utilization data
        self.get_data('total_memory', "max_over_time(cgroup_memory_total_bytes{cluster='%s',jobid='%s',step='',task=''}[%ds])")
        self.get_data('used_memory', "max_over_time(cgroup_memory_used_bytes{cluster='%s',jobid='%s',step='',task=''}[%ds])")
        self.get_data('total_time', "max_over_time(cgroup_cpu_total_seconds{cluster='%s',jobid='%s',step='',task=''}[%ds])")
        self.get_data('cpus', "max_over_time(cgroup_cpus{cluster='%s',jobid='%s',step='',task=''}[%ds])")

        # and now GPUs
        if self.gpus:
            self.get_data('gpu_total_memory', "max_over_time((nvidia_gpu_memory_total_bytes{cluster='%s'} and nvidia_gpu_jobId == %s)[%ds:])")
            self.get_data('gpu_used_memory', "max_over_time((nvidia_gpu_memory_used_bytes{cluster='%s'} and nvidia_gpu_jobId == %s)[%ds:])")
            self.get_data('gpu_utilization', "avg_over_time((nvidia_gpu_duty_cycle{cluster='%s'} and nvidia_gpu_jobId == %s)[%ds:])")

    def human_bytes(self, size, decimal_places=1):
        size=float(size)
        for unit in ['B','KB','MB','GB','TB']:
            if size < 1024.0:
                break
            size /= 1024.0
        return f"{size:.{decimal_places}f}{unit}"

    def human_seconds(self, seconds):
        hour = seconds // 3600
        if hour >= 24:
            days = "%d-" % (hour // 24)
            hour %= 24
            hour = days + ("%02d:" % hour)
        else:
            if hour > 0:
                hour = "%02d:" % hour
            else:
                hour = '00:'
        seconds = seconds % (24 * 3600)
        seconds %= 3600
        minutes = seconds // 60
        seconds %= 60
        return "%s%02d:%02d" % (hour, minutes, seconds)

    def human_datetime(self, x):
       from datetime import datetime
       return datetime.fromtimestamp(x).strftime("%a %b %-d, %Y at %-I:%M %p")

    def simple_output(self):
        term = Terminal()
        gutter = "  "
        # cpu time utilization
        print(f"{gutter}CPU utilization per node (CPU time used/run time)") 
        for node, used, alloc, cores in self.cpu_util__node_used_alloc_cores:
            msg = ""
            if used == 0: msg = f" {term.bold}{term.red}<--- CPU node was not used!{term.normal}"
            print(f"{gutter}    {node}: {self.human_seconds(used)}/{self.human_seconds(alloc)} (efficiency={100 * used / alloc:.1f}%){msg}")
        used, alloc, _ = self.cpu_util_total__used_alloc_cores
        if self.nnodes != "1":
            print(f"{gutter}Total used/runtime: {self.human_seconds(used)}/{self.human_seconds(alloc)}, efficiency={100 * used / alloc:.1f}%")
        # cpu memory usage
        print(f"\n{gutter}CPU memory usage per node - used/allocated")
        for node, used, alloc, cores in self.cpu_mem__node_used_alloc_cores:
            print(f"{gutter}    {node}: {self.human_bytes(used)}/{self.human_bytes(alloc)} ", end="")
            print(f"({self.human_bytes(used*1.0/cores)}/{self.human_bytes(alloc*1.0/cores)} per core of {cores})")
        total_used, total, total_cores = self.cpu_mem_total__used_alloc_cores
        if self.nnodes != "1":
            print(f"{gutter}Total used/allocated: {self.human_bytes(total_used)}/{self.human_bytes(total)} ", end="")
            print(f"({self.human_bytes(total_used*1.0/total_cores)}/{self.human_bytes(total*1.0/total_cores)} per core of {total_cores})")
        if self.gpus:
            # gpu utilization
            print(f"\n{gutter}GPU utilization per node")
            for node, util, gpu_index in self.gpu_util__node_util_index:
                msg = ""
                if util == 0: msg = f" {term.bold}{term.red}<--- GPU was not used!{term.normal}"
                print(f"{gutter}    {node} (GPU {gpu_index}): {util}%{msg}")
            # gpu memory usage
            print(f"\n{gutter}GPU memory usage per node - maximum used/total")
            for node, used, total, gpu_index in self.gpu_mem__node_used_total_index:
                print(f"{gutter}    {node} (GPU {gpu_index}): {self.human_bytes(used)}/{self.human_bytes(total)} ({100.0*used/total:.1f}%)")

    def job_notes(self):
        term = Terminal()
        s = ""
        #### red notes (severe) ###
        zero_gpu = False
        if self.gpus and (self.diff > JobStats.min_runtime_seconds):
            num_unused_gpus = sum([util == 0 for _, util, _ in self.gpu_util__node_util_index])
            if num_unused_gpus:
                if self.gpus == 1:
                    s +=f"  {term.bold}{term.red}* This job did not use the GPU! This is a waste of expensive resources. You\n"
                    s += "    need to resolve this before running additional jobs. Is the code GPU-enabled?\n"
                    s += "    Please consult the documentation for the code. For additional info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{term.normal}\n\n"
                else:
                    s +=f"  {term.bold}{term.red}* This job did not use {num_unused_gpus} of the {self.gpus} allocated GPUs! This is a waste of expensive\n"
                    s += "    resources. You need to resolve this before running additional jobs. Is the\n"
                    s += "    code capable of using multiple GPUs? Please consult the documentation for\n"
                    s += "    the code. For additional info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{term.normal}\n\n"
                zero_gpu = True
        zero_cpu = False
        if (self.diff > JobStats.min_runtime_seconds):
            num_unused_nodes = sum([used == 0 for _, used, _, _ in self.cpu_util__node_used_alloc_cores])
            if num_unused_nodes:
                if self.nnodes == "1":
                    s +=f"  {term.bold}{term.red}* This job did not use the CPU! This suggests that something is going\n"
                    s +=f"    wrong at the very beginning of the job.\n"
                    s +=f"    failing immediately. Please resolve this before submitting additional jobs.{term.normal}\n\n"
                else:
                    s +=f"  {term.bold}{term.red}* This job did not use {num_unused_nodes} of the {self.nnodes} allocated CPU nodes! You need to resolve\n"
                    s +=  "    this before running additional jobs. Is the code capable of using multiple\n"
                    s +=f"    nodes? Please consult the documentation for the code.{term.normal}\n\n"
                zero_cpu = True
        if (not zero_gpu) and self.gpus and (self.gpu_utilization < JobStats.gpu_utilization_red):
            s +=f"  {term.bold}{term.red}* The overall GPU utilization of this job is {round(self.gpu_utilization)}%. Please investigate the reason\n"
            s += "    for the low utilization. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{term.normal}\n\n"
        if (not zero_cpu) and (not self.gpus) and (self.cpu_efficiency < JobStats.cpu_utilization_red):
            s +=f"  {term.bold}{term.red}* The overall CPU utilization of this job is {self.cpu_efficiency}%. This value is low. Please\n"
            if int(self.ncpus) > 1:
                s += "    investigate the reason for the low efficiency. Consider performing a scaling\n"
                s += "    analysis:\n"
                s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis{term.normal}\n\n"
            else:
                s +=f"    investigate the reason for the low efficiency.{term.normal}\n"
        if (self.state == "OUT_OF_MEMORY"):
            s +=f"  {term.bold}{term.red}* This job failed because it exceeded the amount of allocated CPU memory. For\n"
            s +=f"    solutions, see this page:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{term.normal}\n\n"
        if (self.state == "TIMEOUT"):
            s +=f"  {term.bold}{term.red}* This job failed because it exceeded the time limit. The solution is to\n"
            s +=f"    increase the value of the --time directive as explained on this page:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/slurm{term.normal}\n\n"
        total_used, total, total_cores = self.cpu_mem_total__used_alloc_cores
        gb_per_core = round(total / total_cores / 1024**3, 1)
        cpu_memory_utilization = round(100 * total_used / total)
        tense = "is" if self.state == "RUNNING" else "was"
        if (self.cluster == "stellar") and ("pu" in self.qos or "pppl" in self.qos) and (gb_per_core > 9) and \
           (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f"  {term.bold}{term.red}* This job requested {gb_per_core} GB of memory per CPU-core which is more than the\n"
            s +=f"    default value of 8 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s += "    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{term.normal}\n\n"
        if (self.cluster == "stellar") and ("cimes" in self.qos) and (gb_per_core > 5) and \
           (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f"  {term.bold}{term.red}* This job requested {gb_per_core} GB of memory per CPU-core which is more than the\n"
            s +=f"    default value of 4 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s += "    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{term.normal}\n\n"
        if (self.cluster == "traverse") and (gb_per_core > 4) and \
           (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f"  {term.bold}{term.red}* This job requested {gb_per_core} GB of memory per CPU-core which is more than the\n"
            s +=f"    default value of 1.9 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s += "    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{term.normal}\n\n"
        if (self.cluster in ["adroit", "della", "tiger"]) and (not self.partition == "datascience") and \
           (gb_per_core > 5) and (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f"  {term.bold}{term.red}* This job requested {gb_per_core} GB of memory per CPU-core which is more than the\n"
            s +=f"    default value of 4 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s +=f"    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{term.normal}\n\n"
        mem_thres = 380 if self.account == "physics" else 190
        if self.cluster == "della" and self.partition == "datascience" and (total_used / 1024**3 < mem_thres):
            s +=f"  {term.bold}{term.red}* This job ran on a large-memory (datascience) node but it only used {total_used / 1024**3:.1f} GB\n"
            s += "    of CPU memory. The large-memory nodes should only be used for jobs that\n"
            s +=f"    require more than {mem_thres} GB. For future jobs please use an accurate value\n"
            s += "    for the --mem-per-cpu or --mem directive. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{term.normal}\n\n"
        if (self.nnodes == "1") and (self.cluster == "tiger") and (not self.gpus):
            s +=f"  {term.bold}{term.red}* The TigerCPU cluster is intended for multinode jobs. Serial jobs are\n"
            s += "    assigned the lowest priority. On TigerCPU, a serial job is one that runs\n"
            s += "    on 1 node (independent of the number of cores). Consider carrying out this\n"
            s +=f"    work elsewhere. This note does not apply to GPU jobs.{term.normal}\n\n"
        if (self.nnodes == "1") and (self.cluster == "stellar") and (not self.gpus) and (int(self.ncpus) < 48):
            s +=f"  {term.bold}{term.red}* The Stellar cluster is intended for multinode jobs. Serial jobs are assigned\n"
            s += "    the lowest priority. On Stellar, a serial job is one that uses 1 node and\n"
            s += "    less than 48 CPU-cores. Consider carrying out this work elsewhere. This note\n"
            s +=f"    does not apply to GPU jobs.{term.normal}\n\n"
        if self.time_eff_violation:
            s +=f"  {term.bold}{term.red}* The time efficiency of this job is {self.time_efficiency}%. The time efficiency is the run time\n"
            s += "    divided by the time limit. For future jobs please consider decreasing the\n"
            s += "    value of the --time directive to increase the time efficiency. This will\n"
            s += "    lower your queue time and allow the Slurm job scheduler to work more\n"
            s += "    effectively for all users. Jobs with a time limit of less than 62 minutes\n"
            s += "    will land in the test QOS where only a few jobs can run simultaneously. For\n"
            s += "    more info see these pages:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/slurm\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/job-priority#test-queue{term.normal}\n\n"
        ### black notes ###
        if (not zero_cpu) and (self.cpu_efficiency < JobStats.cpu_utilization_black) and (self.cpu_efficiency >= JobStats.cpu_utilization_red) and \
           (not self.gpus):
            s +=f"  * The overall CPU utilization of this job is {self.cpu_efficiency}%. This value is somewhat low.\n"
            if int(self.ncpus) > 1:
                s += "    Please investigate the reason for the low efficiency. Consider performing a\n"
                s += "    scaling analysis:\n" 
                s += "    https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis\n\n"
        if (self.nnodes == "1") and (int(self.ncpus) > 1) and (not self.gpus):
            eff_if_serial = 100 / int(self.ncpus)
            ratio = self.cpu_efficiency / eff_if_serial
            if (ratio > 0.9 and ratio < 1.1):
                s +=f"  * The CPU utilization of this job ({self.cpu_efficiency}%) is approximately equal to 1 divided by\n"
                s +=f"    the number of CPU-cores ({round(100/int(self.ncpus))}%). This suggests that you may be trying to run a\n"
                s += "    serial code using multiple cores.\n\n"
        if (not zero_gpu) and self.gpus and (self.gpu_utilization < JobStats.gpu_utilization_black) and \
           (self.gpu_utilization >= JobStats.gpu_utilization_red):
            s +=f"  * The overall GPU utilization of this job is {round(self.gpu_utilization)}%. This value is low compared\n"
            s += "    to the cluster mean value of about 50%. For more info:\n"
            s += "    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n\n"
        if "test" in self.qos or "debug" in self.qos:
            s +=f"  * This job ran in the {self.qos} QOS. Each user can only run a small number of\n"
            s += "    jobs simultaneously in this QOS. For more info:\n"
            s += "    https://researchcomputing.princeton.edu/support/knowledge-base/job-priority#test-queue\n\n"
        ### default ###
        if (self.cluster == "tiger" or self.cluster == "traverse"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://stats.rc.princeton.edu  (VPN required off-campus)\n\n"
        if (self.cluster == "stellar"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://mystellar.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n"
        if (self.cluster == "della"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://mydella.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n"
        if (self.cluster == "adroit"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://myadroit.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n"
        return s

    def cpu_memory_formatted(self):
        total = self.reqmem.replace("000M", "G").replace("000G", "T").replace(".50G", ".5G").replace(".50T", ".5T")
        if int(self.ncpus) == 1 or all([X not in total for X in ("K", "M", "G", "T")]):
            return f'     CPU Memory: {total.replace("M", "MB").replace("G", "GB").replace("T", "TB")}'
        if total.endswith("K"):
            bytes_ = float(total.replace("K", "")) * 1e3
        elif total.endswith("M"):
            bytes_ = float(total.replace("M", "")) * 1e6
        elif total.endswith("G"):
            bytes_ = float(total.replace("G", "")) * 1e9
        elif total.endswith("T"):
            bytes_ = float(total.replace("T", "")) * 1e12
        else:
            return total
        bytes_per_core = bytes_ / int(self.ncpus)
        for unit in ['B','KB','MB','GB','TB']:
            if bytes_per_core < 1000:
                break
            bytes_per_core /= 1000
        bpc = f"{bytes_per_core:.1f}"
        bpc = bpc.replace(".0", "")
        return f'     CPU Memory: {total.replace("M", "MB").replace("G", "GB").replace("T", "TB")} ({bpc}{unit} per CPU-core)'

    def time_limit_formatted(self):
        self.time_eff_violation = False
        SECONDS_PER_MINUTE = 60
        if self.state == "COMPLETED" and self.timelimitraw > 0:
            self.time_efficiency = round(100 * self.diff / (SECONDS_PER_MINUTE * self.timelimitraw))
            if self.time_efficiency > 100: self.time_efficiency = 100
            clr = ""
            term = Terminal()
            if self.time_efficiency < 50 and self.diff > JobStats.min_runtime_seconds:
                self.time_eff_violation = True
                clr = f"{term.bold}{term.red}"
            return f"     Time Limit: {clr}{self.human_seconds(SECONDS_PER_MINUTE * self.timelimitraw)}{term.normal}"
        else:
            return f"     Time Limit: {self.human_seconds(SECONDS_PER_MINUTE * self.timelimitraw)}"

    def enhanced_output(self):
        term = Terminal()
        print("")
        print(80 * "=")
        print("                              Slurm Job Statistics")
        print(80 * "=")
        print(f"         Job ID: {term.bold}{self.jobid}{term.normal}")
        print(f"  NetID/Account: {self.user}/{self.account}")
        print(f"       Job Name: {self.jobname}")
        if self.state in ("OUT_OF_MEMORY", "TIMEOUT"):
            print(f"          State: {term.bold}{term.red}{self.state}{term.normal}")
        else:
            print(f"          State: {self.state}")
        print(f"          Nodes: {self.nnodes}")
        print(f"      CPU Cores: {self.ncpus}")
        print(self.cpu_memory_formatted())
        if self.gpus:
            print(f"           GPUs: {self.gpus}")
        if self.cluster in ("stellar", "tiger") and (self.partition == "serial"):
            print(f"  QOS/Partition: {self.qos}/{term.bold}{term.red}{self.partition}{term.normal}")
        else:
            print(f"  QOS/Partition: {self.qos}/{self.partition}")
        print(f"        Cluster: {self.cluster}")
        print(f"     Start Time: {self.human_datetime(self.start)}")
        if self.state == "RUNNING":
            print(f"       Run Time: {self.human_seconds(self.diff)} (in progress)")
        else:
            print(f"       Run Time: {self.human_seconds(self.diff)}")
        print(self.time_limit_formatted())
        print("")
        print(f"                              {term.bold}Overall Utilization{term.normal}")
        print(80 * "=")

        def draw_meter(x, hardware, util=False):
          bars = x // 2
          if bars < 0:  bars = 0
          if bars > 50: bars = 50
          text = f"{x}%"
          spaces = 50 - bars - len(text)
          if bars + len(text) > 50:
              bars = 50 - len(text)
              spaces = 0

          clr = clr2 = ""
          if (x < JobStats.cpu_utilization_red and hardware == "cpu" and util and (not self.gpus)) or \
             (x < JobStats.gpu_utilization_red and hardware == "gpu" and util):
              clr  = f"{term.red}"
              clr2 = f"{term.bold}{term.red}"
          return f"{term.bold}[{term.normal}" + clr + bars * "|" + spaces * " " + clr2 + \
                 text + f"{term.normal}{term.bold}]{term.normal}"

        # overall cpu time utilization
        total_used, total, total_cores = self.cpu_util_total__used_alloc_cores
        self.cpu_efficiency = round(100 * total_used / total)
        print("  CPU utilization  " + draw_meter(self.cpu_efficiency, "cpu", util=True))
        # overall cpu memory utilization
        total_used, total, total_cores = self.cpu_mem_total__used_alloc_cores
        cpu_memory_efficiency = round(100 * total_used / total)
        print("  CPU memory usage " + draw_meter(cpu_memory_efficiency, "cpu"))
        if self.gpus:
            # overall gpu utilization
            overall, overall_gpu_count = self.gpu_util_total__util_gpus
            self.gpu_utilization = overall / overall_gpu_count
            print("  GPU utilization  " + draw_meter(round(self.gpu_utilization), "gpu", util=True))
            # overall gpu memory usage
            overall, overall_total = self.gpu_mem_total__used_alloc
            gpu_memory_usage = round(100 * overall / overall_total)
            print("  GPU memory usage " + draw_meter(gpu_memory_usage, "gpu"))
        print()
        print(f"                              {term.bold}Detailed Utilization{term.normal}")
        print(80 * "=")
        self.simple_output()
        print()
        #import pdb; pdb.set_trace()
        notes = self.job_notes()
        if notes:
            print(f"                                     {term.bold}Notes{term.normal}")
            print(80 * "=")
            print(notes)


    def report_job(self):
        sp_node = self.sp_node

        if len(sp_node)==0:
            if self.diff < 30:
                self.error("The job %s is too short (%s seconds) for meaningful detailed statistics, please use seff command." % (self.jobid, self.diff))
            else:
                self.error(f"I found no stats for job {self.jobid}, either because it is too old or because\n"
                          + "it expired from jobstats database. If you are not running this command on the\n"
                          + "cluster where the job was run then use the -c option to specify the cluster.")

        # cpu utilization
        total = 0
        total_used = 0
        total_cores = 0
        self.cpu_util__node_used_alloc_cores = []
        for n in sp_node:
            used = sp_node[n]['total_time']
            cores = sp_node[n]['cpus']
            alloc = self.diff * cores
            total += alloc
            total_used += used
            total_cores += cores
            self.cpu_util__node_used_alloc_cores.append((n, used, alloc, cores))
        self.cpu_util_total__used_alloc_cores = (total_used, total, total_cores)

        # cpu memory
        total = 0
        total_used = 0
        total_cores = 0
        self.cpu_mem__node_used_alloc_cores = []
        for n in sp_node:
            used = sp_node[n]['used_memory']
            alloc = sp_node[n]['total_memory']
            cores = sp_node[n]['cpus']
            total += alloc
            total_used += used
            total_cores += cores
            self.cpu_mem__node_used_alloc_cores.append((n, used, alloc, cores))
        self.cpu_mem_total__used_alloc_cores = (total_used, total, total_cores)

        if self.gpus:
            # gpu utilization
            overall = 0
            overall_gpu_count = 0
            self.gpu_util__node_util_index = []
            for n in sp_node:
                d = sp_node[n]
                gpus = list(d['gpu_utilization'].keys())
                gpus.sort()
                for g in gpus:
                    util = d['gpu_utilization'][g]
                    overall += util
                    overall_gpu_count += 1
                    self.gpu_util__node_util_index.append((n, util, g))
            self.gpu_util_total__util_gpus = (overall, overall_gpu_count)

            # gpu memory usage
            overall = 0
            overall_total = 0
            self.gpu_mem__node_used_total_index = []
            for n in sp_node:
                d = sp_node[n]
                gpus = list(d['gpu_total_memory'].keys())
                gpus.sort()
                for g in gpus:
                    used = d['gpu_used_memory'][g]
                    total = d['gpu_total_memory'][g]
                    overall += used
                    overall_total += total
                    self.gpu_mem__node_used_total_index.append((n, used, total, g))
            self.gpu_mem_total__used_alloc = (overall, overall_total)

        self.simple_output() if args.simple else self.enhanced_output()

 
    def __str__(self, compact=False):
        js_data = { 'nodes': self.sp_node, 'total_time': self.diff, 'gpus': self.gpus }
        if compact:
            return json.dumps(js_data, separators=(',', ':'))
        else:
            return json.dumps(js_data, sort_keys=True, indent=4)

    def report_job_json(self, encode):
        data = self.__str__(encode)
        if encode:
            return base64.b64encode(gzip.compress(data.encode('ascii'))).decode('ascii')
        else:
            return data

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Show job utilization.")
    parser.add_argument('job', metavar='jobid', nargs='+',
                    help='Job numbers to lookup')
    parser.add_argument("-c", "--cluster", default=None,
                    help="Specify cluster instead of relying on default on the current machine.")
    parser.add_argument("-j", "--json", action='store_true', default=False,
                    help="Produce row data in json format, with no summary.")
    parser.add_argument("-b", "--base64", action='store_true', default=False,
                    help="Produce row data in json format, with no summary and also gzip and encode it in base64 output for db storage.")
    parser.add_argument("-d", "--debug", action='store_true', default=False,
                    help="Output debugging information.")
    parser.add_argument("-s", "--simple", action='store_true', default=False,
                    help="Output information using a simple format.")
    args = parser.parse_args()

    if args.cluster == "tiger": args.cluster = "tiger2"
    for jobid in args.job:
        #import pdb; pdb.set_trace()
        stats = JobStats(jobid=jobid, cluster=args.cluster, debug=args.debug)
        if args.json or args.base64:
            print(stats.report_job_json(args.base64))
        else:
            stats.report_job()
