\section{Tools of the Jobstats Platform}
\label{jobstats_tools}
The Prometheus and Slurm databases provide a rich dataset that can be harnessed by special-purpose tools:

\begin{itemize}
\item {\verb|jobstats|}: A command for generating a detailed Slurm efficiency report for a given job.
\item {\verb|job defense shield|}: A tool for sending automated email alerts to users.
\item {\verb|gpudash|}: A command that generates a dashboard of the utilization of each GPU on the cluster.
\item {\verb|reportseff|}: A command for displaying a simple Slurm efficiency report for several jobs at once.
\item {\verb|utilization reports|}: A tool for sending detailed usage reports to users by email.
\end{itemize}

\noindent
More details about each of these tools are provided below.

\subsection{jobstats}
The \texttt{jobstats} command provides users with a Slurm job efficiency report. For completed jobs, the data is taken from a call to \texttt{sacct} with several fields including \texttt{AdminComment}. For running jobs, the Prometheus database must be queried using the following:

\begin{verbatim}
    max_over_time(cgroup_memory_total_bytes{...}[...])
    max_over_time(cgroup_memory_rss_bytes{...}[...])
    max_over_time(cgroup_cpu_total_seconds{...}[...])
    max_over_time(cgroup_cpus{...}[...])
\end{verbatim}

\noindent
The following additional queries are needed for actively running GPU jobs:

\begin{verbatim}
    max_over_time((nvidia_gpu_memory_total_bytes{cluster=c} and nvidia_gpu_jobId == j)[...])
    max_over_time((nvidia_gpu_memory_used_bytes{cluster=c} and nvidia_gpu_jobId == j)[...])
    avg_over_time((nvidia_gpu_duty_cycle{cluster=c} and nvidia_gpu_jobId == j)[...])
\end{verbatim}

The \texttt{jobstats} command requires a jobid:

\begin{verbatim}
    $ jobstats 247463
\end{verbatim}

\noindent
An example of the \texttt{jobstats} output is available at \url{https://github.com/PrincetonUniversity/jobstats}. The first part of the output displays job metadata such as the username, account, partition, cluster, number of CPU-cores, start time and so on. The second part uses a text-based meter to indicate the overall CPU utilization and CPU memory usage (with the analogous information also being shown for GPU jobs). Detailed information is provided in the third part of the output which includes per-node, per-CPU and per-GPU values for the utilization and memory usage. The final panel includes useful notes for the user based on the job metadata and the CPU and GPU efficiencies and memory used. Below is an example note from a job that over-allocated CPU memory:

\texttt{* This job requested 64 GB of CPU memory. Given that the CPU memory usage was only 5 GB or 8\% of the total, please consider reducing your CPU memory allocation for future jobs by modifying the -{}-mem or -{}-mem-per-cpu Slurm directives. This will reduce your queue times and make the resources available to other users. For more:
https://researchcomputing.princeton.edu/memory}

For our institution, there are currently more than twenty possible notes. They cover such issues as low CPU or GPU utilization, over-allocating CPU memory, using an excessive run time limit, allocating more nodes than necessary (i.e., job fragmentation), and running jobs in the test queue. If a job runs for less than twice the sampling period of the Prometheus exporters then the \texttt{seff} command is used in place of \texttt{jobstats}. Color and bold font is used throughout the report to draw the user's attention to key pieces of information.

Importantly, the \texttt{jobstats} command is also used to for Slurm efficiency reports that are sent by email after a job completes. This is done by chaning the \texttt{MailProg} setting in \texttt{slurm.conf}. For details see Appendix~\ref{replace_seff}.

The installation requirements for \texttt{jobstats} are Python 3.6+ and version 1.17+ of the Python \texttt{blessed} package~\cite{rose} which is used for coloring and styling text. The Python code and instructions are available in the GitHub repository~\cite{jobstats_repo}.


\subsection{Job Defense Shield}
High-performance computing clusters often serve a large number of users who posses a range of knowledge and skills. This leads to individuals misusing the resources due to mistakes, misunderstandings, expediency, and related issues. To combat jobs that waste or misuse the resources, a battery of alerts can be configured. While such alerts can be configured in Prometheus~\cite{prometheus}, the most flexible and powerful solution is external software.

The \texttt{job defense shield} is a Python code for sending automated email alerts to users and for creating reports for system administrators. As discussed above, summary statistics for each completed job are stored in a compressed format in the \texttt{AdminComment} field in the Slurm database. The software described here works by calling \texttt{sacct} while requesting several fields including \texttt{AdminComment}. The \texttt{sacct} output is stored in a \texttt{pandas} dataframe for processing.

The \texttt{job denfense shield} provides email alerts for the following:

\begin{itemize}
\item actively running jobs where a CPU or GPU has zero utilization over some time window
\item users in the top $N$ by usage over some time window with low CPU or GPU utilization
\item jobs that could have been run on a less powerful GPU (e.g., an NVIDIA V100 versus A100)
\item users that have been over-allocating CPU or GPU time
\item queued jobs that request too many nodes (e.g., 1 CPU-core per node over 10 nodes)
\item jobs that request much more than the default CPU memory but do not use it.
\end{itemize}

The Python code is written using object-oriented programming techniques which makes it easy to create new alerts. A new alert is made by writing a new class that derives from the \texttt{Alert} abstract base class.

The \texttt{job denfense shield} has a \texttt{check} mode that shows on which days a given user received an alert of a given type. Users that appear to be ignoring the email alerts can be contacted directly. Emails to users are most effective when sent sparingly. For this reason, there is a command-line parameter \texttt{-{}-days-between-emails} that specifies the amount of time that must pass before the user can receive another email of the same nature.

The example below shows how the script is called to notify users in the top $N$ by usage with low CPU or GPU efficiencies over the last week:

\begin{verbatim}
    $ python job_defense_shield.py --days=7 --low-xpu-efficiencies --email
\end{verbatim}

\noindent
The default thresholds are 60\% and 15\% for CPU and GPU utilization, respectively, and $N=15$.

The installation requirements for the \texttt{job defense shield} are Python 3.6+ and version 1.2+ of the Python \texttt{pandas} package~\cite{reback2020pandas}. The \texttt{jobstats} command is also required if one wants to examine actively running jobs such as when looking for jobs with zero GPU utilization. The Python code, example alerts and emails, and instructions are available at \url{https://github.com/PrincetonUniversity/job_defense_shield}.



\subsection{gpudash}

The \texttt{gpudash} command generates a text-based dashboard of the GPU utilization across a cluster in the form of a 2-dimensional grid. Each cell displays the utilization from 0-100\% along with the username associated with each allocated GPU. Cells are colored according to their utilization values making it easy to identify jobs with low or high GPU utilization. The \texttt{gpudash} command is also used by users to check for available GPUs.

By default, the dashboard has seven columns and a number of rows equal to the number of GPUs on the cluster. Each column is evenly spaced in time by $N$ minutes. We find a good choice is $N=10$ minutes which leads to data being shown over an hour. The \texttt{cron} utility can be used to achieve this. The rows are labeled by the node name and the GPU index while the columns are labeled by time.

The \texttt{gpudash} command works by making the following three queries to the Prometheus server every $N$ minutes:

\begin{verbatim}
  curl -s 'http://prometheus-server:9090/api/v1/query?query=nvidia_gpu_duty_cycle' > util.json
  curl -s 'http://prometheus-server:9090/api/v1/query?query=nvidia_gpu_jobUid'     > uids.json
  curl -s 'http://prometheus-server:9090/api/v1/query?query=nvidia_gpu_jobId'      > jobids.json
\end{verbatim}

\noindent
A Python script is used to extract the information from the three generated JSON files and append this data to the so-called column files read by \texttt{gpudash}. The UID for each user is matched with its corresponding username. The jobid is not required but it can be useful for troubleshooting.

Nodes that are down, or in a state which makes them unavailable, are not shown in the visualization. Special labels can be added to mark reserved nodes or special-purpose nodes. The installation requirements for \texttt{gpudash} are Python 3.6+ and version 1.17+ of the Python \texttt{blessed} package~\cite{rose} which is used for creating colored text and backgrounds. The Python code and instructions are available at \url{https://github.com/PrincetonUniversity/gpudash}.



\subsection{reportseff}

\texttt{reportseff} wraps sacct to provide a cleaner user experience when interrogating Slurm job efficiency values for multiple jobs. In addition to multiple job ids, \texttt{reportseff} accepts Slurm output files as arguments and parses the job id from the filename.  Some \texttt{sacct} options are further wrapped or extended to simplify common operations.  The output is a table with entries colored based on high/low utilization values.  The columns and formatting of the table can be customized based on command line options.

A limit to the previous tools is that they provide information on a single job at a time in great detail.  Another common use case is to summarize job efficiency for multiple jobs to gain a better idea of the overall utilization.  Summarized reporting is especially useful with array jobs and workflow managers which interface with Slurm.  In these cases, running \texttt{seff} or \texttt{jobstats} becomes burdensome.  \texttt{reportseff} accepts jobs as job ids, slurm output files, and directories containing Slurm output files
\begin{verbatim}
    reportseff 123 124  # get information on jobs 123 and 124
    reportseff {123..133}  # get information on jobs 123 to 133
    reportseff jobname*  # check output files starting with jobname
    reportseff slurm_out/  # look for output files in the slurm_out directory
\end{verbatim}
The ability to link Slurm outputs with job status simplifies locating problematic jobs and cleaning up their outputs.

\texttt{reportseff} extends some \texttt{sacct} options.  The start and end time can accept any format accepted by \texttt{sacct}, as well as a custom format, specified as a comma separated list of key/value pairs.  For example:
\begin{verbatim}
    reportseff --since now-27hours  # equivalent to
    reportseff --since d=1,h=3  # 1 day, 3 hours
\end{verbatim}
Filtering by job state is expanded with \texttt{reportseff} to specify states to exclude.  This filtering combined with accepting output files helps in cleaning up failed output jobs
\begin{verbatim}
    reportseff \
      --not-state CD \  # not completed
      --since d=1 \  # today
      --format=jobid \  # just get file name, suitable for piping
      my_failing_job* \  # only from these outputs
      | xargs grep "output:"  # find lines with the output directive to examine or delete
\end{verbatim}
The format option can accept a comma separated list of column names or additional columns can be appended to the default values.  Appending prevents the need to add in the same, default columns on every invocation.

While the above features are available for any Slurm system, when jobstats information is present in the \texttt{AdminComment}, the multi-node resource utilization is updated with the more accurate jobstats values and GPU utilization is also provided.  This additional information is controlled with the \texttt{-{}-node} and \texttt{-{}-node-and-gpu} options.

A sample workflow with \texttt{reportseff} is to run a series of jobs, each producing an output file.  Run \texttt{reportseff} on the output directory to determine the utilization and state of each job.  Jobs with low utilization or failure can be examined more closely by copy/pasting the Slurm output filename from the first column.  Outputs from failed jobs can be cleaned automatically with a version of the command piping above. Combining with \texttt{watch} and aliases can create powerful monitoring for users:
\begin{verbatim}
    # monitor the current directory, every 5 minutes
    watch -cn 300 reportseff --modified-sort
    # monitor the user's efficiency every 10 minutes.  Defaults to last week of jobs
    watch -cn 600 reportseff --user $USER --modified-sort --format=+jobname
\end{verbatim}

The installation requirements for \texttt{reportseff} are Python 3.7+ and version 6.7+ of the Python \texttt{click} package which is used for creating colored text and command line parsing. The Python code and instructions are available at \url{https://github.com/troycomi/reportseff}.



\subsection{Utilization Reports}
Users can choose to receive an email utilization report upon completion of each Slurm job. Because some users decide not to receive these emails, it is important to periodically send a comprehensive utilization report to each user. As discussed above, summary statistics for each completed job are stored in a compressed format in the \texttt{AdminComment} field in the Slurm database. The software described here works by calling \texttt{sacct} while requesting several fields including \texttt{AdminComment}. The \texttt{sacct} output is stored in a \texttt{pandas} dataframe for processing.

Each user that ran at least one Slurm job in the specified time interval will receive a report when the software is run. The first part of the report is a table that indicates the overall usage for each cluster. Each row provides the CPU-hours, GPU-hours, number of jobs, Slurm account(s) and partition(s) that were used by the user.

The second part of the report is a detailed table showing for each partition of each cluster the CPU-hours, CPU-rank, CPU-eff, GPU-hours, GPU-rank, GPU-eff and number of jobs. The CPU-rank or GPU-rank indicates the user's usage relative to the other users on the given partition of the cluster. CPU-eff (or GPU-eff) is the overall CPU (or GPU) efficiency which varies from 0-100\%. A responsible user will take action when seeing that their rank is high while their efficiency is low. The email report also provides a definition for each reported quantity. The software could be extended by adding queue hours and storage information to the tables.

The default mode of the software is to send user reports. It can also be used to send reports to those that are responsible for the users such as the principal investigator. This is the so-called \texttt{sponsors} mode. The example below shows how the script is called to generate user reports over the past month which are sent by email:

\begin{verbatim}
    $ python utilization_reports.py --report-type=users --months=1 --email
\end{verbatim}

We find a good choice is to send the report once per month. The installation requirements for the software are Python 3.6+ and version 1.2+ of the \texttt{pandas} package~\cite{reback2020pandas}. The Python code, example reports, and instructions are available at \url{https://github.com/PrincetonUniversity/monthly_sponsor_reports}.
